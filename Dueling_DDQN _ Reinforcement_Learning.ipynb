{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9jOGv7LRVoFRAOsSD2QC+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"toM5I_lIghi5","executionInfo":{"status":"ok","timestamp":1720461954111,"user_tz":-300,"elapsed":221432,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"d4da96a7-71db-45c7-bdd1-2f20a2576ab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Collecting atari-py\n","  Downloading atari-py-0.2.9.tar.gz (540 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.6/540.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atari-py) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-0.2.9-cp310-cp310-linux_x86_64.whl size=2870889 sha256=e9ac7517860d183df339fa0792a094f49da210d1fb7ba6c934ae0ff62de5470e\n","  Stored in directory: /root/.cache/pip/wheels/75/6f/04/1f3bf5255580101e16ff487564354dddcdd23ec3b43b775b7a\n","Successfully built atari-py\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, atari-py, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed atari-py-0.2.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install numpy torch matplotlib opencv-python atari-py"]},{"cell_type":"code","source":["import os\n","import collections\n","\n","import gym\n","import numpy as np\n","\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"metadata":{"id":"BboianH9goVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DuelingDeepQNetwork(nn.Module):\n","    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n","        super(DuelingDeepQNetwork, self).__init__()\n","\n","        self.checkpoint_dir = chkpt_dir\n","        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n","\n","        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n","\n","        fc_input_dims = self.calculate_conv_output_dims(input_dims)\n","\n","        self.fc1 = nn.Linear(fc_input_dims, 1024)\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.V = nn.Linear(512, 1)\n","        self.A = nn.Linear(512, n_actions)\n","\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n","        self.loss = nn.MSELoss()\n","        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","\n","    def calculate_conv_output_dims(self, input_dims):\n","        state = T.zeros(1, *input_dims)\n","        dims = self.conv1(state)\n","        dims = self.conv2(dims)\n","        dims = self.conv3(dims)\n","        return int(np.prod(dims.size()))\n","\n","    def forward(self, state):\n","        conv1 = F.relu(self.conv1(state))\n","        conv2 = F.relu(self.conv2(conv1))\n","        conv3 = F.relu(self.conv3(conv2))\n","        conv_state = conv3.view(conv3.size()[0], -1)\n","        flat1 = F.relu(self.fc1(conv_state))\n","        flat2 = F.relu(self.fc2(flat1))\n","\n","        V = self.V(flat2)\n","        A = self.A(flat2)\n","\n","        return V, A\n","\n","    def save_checkpoint(self):\n","#         print('... saving checkpoint ...')\n","        T.save(self.state_dict(), self.checkpoint_file)\n","\n","    def load_checkpoint(self):\n","#         print('... loading checkpoint ...')\n","        self.load_state_dict(T.load(self.checkpoint_file))"],"metadata":{"id":"5ojaF29sgrQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer(object):\n","    def __init__(self, max_size, input_shape, n_actions):\n","        self.mem_size = max_size\n","        self.mem_cntr = 0\n","        self.state_memory = np.zeros((self.mem_size, *input_shape),\n","                                     dtype=np.float32)\n","        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n","                                         dtype=np.float32)\n","\n","        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n","        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_cntr % self.mem_size\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.action_memory[index] = action\n","        self.reward_memory[index] = reward\n","        self.terminal_memory[index] = done\n","\n","        self.mem_cntr += 1\n","\n","    def sample_buffer(self, batch_size):\n","        max_mem = min(self.mem_cntr, self.mem_size)\n","        batch = np.random.choice(max_mem, batch_size, replace=False)\n","\n","        states = self.state_memory[batch]\n","        actions = self.action_memory[batch]\n","        rewards = self.reward_memory[batch]\n","        states_ = self.new_state_memory[batch]\n","        terminal = self.terminal_memory[batch]\n","\n","        return states, actions, rewards, states_, terminal"],"metadata":{"id":"6BOsovuvg4Hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DuelingDDQNAgent(object):\n","    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n","                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n","                 replace=1000,  algo=None, env_name=None, chkpt_dir='tmp/dqn'):\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.lr = lr\n","        self.n_actions = n_actions\n","        self.input_dims = input_dims\n","        self.batch_size = batch_size\n","        self.eps_min = eps_min\n","        self.eps_dec = eps_dec\n","        self.replace_target_cnt = replace\n","        self.algo = algo\n","        self.env_name = env_name\n","        self.chkpt_dir = chkpt_dir\n","        self.action_space = [i for i in range(n_actions)]\n","        self.learn_step_counter = 0\n","\n","        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n","\n","        self.q_eval = DuelingDeepQNetwork(self.lr, self.n_actions,\n","                        input_dims=self.input_dims,\n","                        name=self.env_name+'_'+self.algo+'_q_eval',\n","                        chkpt_dir=self.chkpt_dir)\n","        self.q_next = DuelingDeepQNetwork(self.lr, self.n_actions,\n","                        input_dims=self.input_dims,\n","                        name=self.env_name+'_'+self.algo+'_q_next',\n","                        chkpt_dir=self.chkpt_dir)\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        self.memory.store_transition(state, action, reward, state_, done)\n","\n","    def sample_memory(self):\n","        state, action, reward, new_state, done = \\\n","                                self.memory.sample_buffer(self.batch_size)\n","\n","        states = T.tensor(state).to(self.q_eval.device)\n","        rewards = T.tensor(reward).to(self.q_eval.device)\n","        dones = T.tensor(done).to(self.q_eval.device)\n","        actions = T.tensor(action).to(self.q_eval.device)\n","        states_ = T.tensor(new_state).to(self.q_eval.device)\n","\n","        return states, actions, rewards, states_, dones\n","\n","    def choose_action(self, observation):\n","        if np.random.random() > self.epsilon:\n","            state = np.array([observation], copy=False, dtype=np.float32)\n","            state_tensor = T.tensor(state).to(self.q_eval.device)\n","            _, advantages = self.q_eval.forward(state_tensor)\n","\n","            action = T.argmax(advantages).item()\n","        else:\n","            action = np.random.choice(self.action_space)\n","\n","        return action\n","\n","    def replace_target_network(self):\n","\n","        if self.replace_target_cnt is not None and \\\n","           self.learn_step_counter % self.replace_target_cnt == 0:\n","            self.q_next.load_state_dict(self.q_eval.state_dict())\n","\n","    def decrement_epsilon(self):\n","        self.epsilon = self.epsilon - self.eps_dec \\\n","                           if self.epsilon > self.eps_min else self.eps_min\n","\n","    def learn(self):\n","        if self.memory.mem_cntr < self.batch_size:\n","            return\n","\n","        self.q_eval.optimizer.zero_grad()\n","\n","        self.replace_target_network()\n","\n","        states, actions, rewards, states_, dones = self.sample_memory()\n","        indices = np.arange(self.batch_size)\n","\n","        V_s, A_s = self.q_eval.forward(states)\n","\n","        V_s_, A_s_ = self.q_next.forward(states_)\n","\n","        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n","\n","        q_pred = T.add(V_s,\n","                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]\n","\n","        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n","\n","        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1,keepdim=True)))\n","\n","        max_actions = T.argmax(q_eval, dim=1)\n","        q_next[dones] = 0.0\n","\n","        q_target = rewards + self.gamma*q_next[indices, max_actions]\n","\n","        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n","        loss.backward()\n","        self.q_eval.optimizer.step()\n","\n","        self.learn_step_counter += 1\n","\n","        self.decrement_epsilon()\n","\n","    def save_models(self):\n","        self.q_eval.save_checkpoint()\n","        self.q_next.save_checkpoint()\n","\n","    def load_models(self):\n","        self.q_eval.load_checkpoint()\n","        self.q_next.load_checkpoint()\n"],"metadata":{"id":"dhw7KhL0g-XT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RepeatActionAndMaxFrame(gym.Wrapper):\n","    \"\"\" modified from:\n","        https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n","    \"\"\"\n","    def __init__(self, env=None, repeat=4):\n","        super(RepeatActionAndMaxFrame, self).__init__(env)\n","        self.repeat = repeat\n","        self.shape = env.observation_space.low.shape\n","        self.frame_buffer = np.zeros_like((2,self.shape))\n","\n","    def step(self, action):\n","        t_reward = 0.0\n","        done = False\n","        for i in range(self.repeat):\n","            obs, reward, done, info = self.env.step(action)\n","            t_reward += reward\n","            idx = i % 2\n","            self.frame_buffer[idx] = obs\n","            if done:\n","\n","                break\n","\n","        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n","        return max_frame, t_reward, done, info\n","\n","    def reset(self):\n","        obs = self.env.reset()\n","        self.frame_buffer = np.zeros_like((2,self.shape))\n","        self.frame_buffer[0] = obs\n","        return obs\n","\n","class PreprocessFrame(gym.ObservationWrapper):\n","    def __init__(self, shape, env=None):\n","        super(PreprocessFrame, self).__init__(env)\n","        self.shape=(shape[2], shape[0], shape[1])\n","        self.observation_space = gym.spaces.Box(low=0, high=1.0,\n","                                              shape=self.shape,dtype=np.float32)\n","    def observation(self, obs):\n","        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n","        resized_screen = cv2.resize(new_frame, self.shape[1:],\n","                                    interpolation=cv2.INTER_AREA)\n","\n","        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n","        new_obs = np.swapaxes(new_obs, 2,0)\n","        new_obs = new_obs / 255.0\n","        return new_obs\n","\n","class StackFrames(gym.ObservationWrapper):\n","    def __init__(self, env, n_steps):\n","        super(StackFrames, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(\n","                             env.observation_space.low.repeat(n_steps, axis=0),\n","                             env.observation_space.high.repeat(n_steps, axis=0),\n","                             dtype=np.float32)\n","        self.stack = collections.deque(maxlen=n_steps)\n","\n","    def reset(self):\n","        self.stack.clear()\n","        observation = self.env.reset()\n","        for _ in range(self.stack.maxlen):\n","            self.stack.append(observation)\n","\n","        return np.array(self.stack).reshape(self.observation_space.low.shape)\n","\n","    def observation(self, observation):\n","        self.stack.append(observation)\n","        obs = np.array(self.stack).reshape(self.observation_space.low.shape)\n","\n","        return obs\n","\n","def make_env(env_name, shape=(84,84,1), skip=4):\n","    env = gym.make(env_name)\n","    env = RepeatActionAndMaxFrame(env, skip)\n","    env = PreprocessFrame(shape, env)\n","    env = StackFrames(env, skip)\n","\n","    return env\n"],"metadata":{"id":"LzJH593IhpKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","\n","env = make_env('PongNoFrameskip-v4')\n","best_score = -np.inf\n","load_checkpoint = False\n","n_games = 600\n","\n","chkpt_dir = os.path.join(os.getcwd(), 'models')\n","os.mkdir(chkpt_dir)\n","\n","agent = DuelingDDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,\n","                 input_dims=(env.observation_space.shape),\n","                 n_actions=env.action_space.n, mem_size=50000, eps_min=0.1,\n","                 batch_size=32, replace=10000, eps_dec=1e-5,\n","                 chkpt_dir=chkpt_dir, algo='DuelingDDQNAgent',\n","                 env_name='PongNoFrameskip-v4')\n","\n","if load_checkpoint:\n","    agent.load_models()\n","\n","n_steps = 0\n","scores, eps_history, steps_array = [], [], []\n","\n","for i in range(n_games):\n","    done = False\n","    observation = env.reset()\n","\n","    score = 0\n","    while not done:\n","        action = agent.choose_action(observation)\n","        observation_, reward, done, info = env.step(action)\n","        score += reward\n","\n","        if not load_checkpoint:\n","            agent.store_transition(observation, action,\n","                                 reward, observation_, int(done))\n","            agent.learn()\n","        observation = observation_\n","\n","        n_steps += 1\n","    scores.append(score)\n","    steps_array.append(n_steps)\n","\n","    avg_score = np.mean(scores[-100:])\n","    print('episode: ', i,'score: ', score,\n","         ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n","        'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n","\n","    if avg_score > best_score:\n","        if not load_checkpoint:\n","            agent.save_models()\n","        best_score = avg_score\n","\n","    eps_history.append(agent.epsilon)\n","    if load_checkpoint and n_steps >= 18000:\n","        break\n","\n","x = [i+1 for i in range(len(scores))]\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n","    fig=plt.figure()\n","    ax=fig.add_subplot(111, label=\"1\")\n","    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n","\n","    ax.plot(x, epsilons, color=\"C0\")\n","    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n","    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n","    ax.tick_params(axis='x', colors=\"C0\")\n","    ax.tick_params(axis='y', colors=\"C0\")\n","\n","    N = len(scores)\n","    running_avg = np.empty(N)\n","    for t in range(N):\n","\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n","\n","    ax2.scatter(x, running_avg, color=\"C1\")\n","    ax2.axes.get_xaxis().set_visible(False)\n","    ax2.yaxis.tick_right()\n","    ax2.set_ylabel('Score', color=\"C1\")\n","    ax2.yaxis.set_label_position('right')\n","    ax2.tick_params(axis='y', colors=\"C1\")\n","\n","    if lines is not None:\n","        for line in lines:\n","            plt.axvline(x=line)\n","\n","    plt.savefig(filename)\n","\n","fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' \\\n","    + str(n_games) + 'games'\n","figure_file = os.path.join(os.getcwd(), 'plots', fname + '.png')\n","os.mkdir(os.path.join(os.getcwd(), 'plots'))\n","plot_learning_curve(steps_array, scores, eps_history, figure_file)"],"metadata":{"id":"UaZylFdih3vq"},"execution_count":null,"outputs":[]}]}